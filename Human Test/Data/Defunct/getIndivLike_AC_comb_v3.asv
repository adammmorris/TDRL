%% getIndivLike_AC
% This is our actor-critic model.

%% Params
% x: [alphaR alphaP betaR betaP temp stay eligR eligP] (length=8)
% actions: [A1 A2] or [A1 A2 A3]
%   should all be 1 or 2
% states: [1 S2] or [1 S2 S3]
%   2nd column should be 2 or 3, 3rd column (if it exists) should be 4-7
% rewards: [0 0 re]
%   reward received for each move in each round (should be between -5 and 5)
% round: the round # for each row
% comb: set to 1 if you want to use the combined model, 0 if you want to
%   use the uncombined

%% Versions
% v1.2: getting rid of slips
% v1.3: changing normalization to real instead of just for softmax
% v1.4: adding optional eligibility traces, making two normalization options
% v1.5: oh god, fixed an ugly bug
% v2 (7/28/2014): took out type, adding 'comb' parameter, making numLevels fluid

function [likelihood] = getIndivLike_AC_comb_v3(x, actions, states, rewards, roundNum, comb)

%% ENVIRONMENT PARAMETERS

% Board properties.
% Board variables we get from the boards file:
%   boards - boards(state_i, board_k) = the reward from state_i in board_k
%   transitions - transitions(state_i, action_j, board_k) is the state arrived at after
%       taking action_j at state_i in board_k.
%   numStates (# of states)
%   numMoves (# of moves each agent makes)
%   numActions (# of choices at each agent decision point)
%   numBoards (number of agent-board systems to be instantiated and played
%       through separately)

%[currentPath, ~, ~] = fileparts(mfilename('fullpath'));
%load(strcat(currentPath, '/Boards/', boardPath, '.mat'));

% Info about his board
numMoves = size(actions,2);
numStates = 2^(numMoves+1);
numActions = 2;
practiceCutoff = 25;

% Data variables:
% id, A1, S2, A2, Re

%% AGENT PARAMETERS

% Set gammas, alphas, betas, temps

alphaR = x(1);
alphaP = x(2);
betaR = x(3);
betaP = x(4);
temp = x(5);
stay = x(6);
eligR = x(7);
eligP = x(8);
gammaR = .85;
gammaP = .85;

% Set up initial state/action preference matrix (actor) and initial value
%   matrix (critic)
policy0 = zeros(numStates,numActions);
values0 = zeros(numStates, 1);

%% PLAY THE BOARD

% Calculate likelihoods
likelihood = 0;

if comb==1
    policy = policy0;
    values = values0;
else
    policyR = policy0;
    policyP = policy0;
    valuesR = values0;
    valuesP = values0;
end

prevActions = zeros(1,numMoves);

% Loop through each of the rounds
for thisRound = 1:length(A1)
    % Only do rounds that aren't practice rounds
    if roundNum(thisRound) > practiceCutoff
        for thisMove = 1:numMoves
            % GET WHAT HAPPENED
            
            state = states(thisRound,thisMove);
            action = actions(thisRound,thisMove);
            % For newstate, if it's the last move we have to calculate it
            if thisMove < numMoves, newstate = curStates(thisMove+1);
            else newstate = state*numActions+action-1; end
            reward = rewards(thisRound,thisMove);
            
            % MAKE MOVE
            
            % If combined, use policy; if not, add policies
            temppolicy = policy*(comb==1)+(policyR+policyP)*(comb==0);
            % Give stay bonus
            if prevActions(thisMove), temppolicy(state,prevActions(thisMove)) = policy(state,prevActions(thisMove))+stay; end
            % Do move
            probs = softmax_TDRL(temp,temppolicy(state,:),0);
            % Update likelihood
            likelihood = likelihood + log(probs(action));
            
            % UPDATE MODEL FREE
            
            % Combined model?
            if comb == 1
                % Reward? Use r parameters
                if reward >= 0
                    alpha = alphaR;
                    beta = betaR;
                    elig = eligR;
                    gamma = gammaR;
                else % Otherwise use p parameters
                    alpha = alphaP;
                    beta = betaP;
                    elig = eligP;
                    gamma = gammaP;
                end
                
                % Do main update
                delta = reward + gamma*values(newstate) - values(state);
                policy(state,action) = policy(state,action) + beta*delta;
                values(state) = values(state) + alpha*delta;
                
                % Do eligiblity traces
                % Walk backwards through moves
                for i=(thisMove-1):-1:1
                    eligstate = states(thisRound,i);
                    eligaction = actions(thisRound,i);
                    policy(eligstate,eligaction) = policy(eligstate,eligaction) + (elig^i)*beta*delta;
                    values(eligstate) = values(eligstate) + (elig^i)*alpha*delta;
                end
            else % Uncombined model?
                deltaR = reward + gammaR*values(newstate) - values(sta
            end
            
            % First move - no reward
            delta = gammaR .* values(state2) - values(1);
            policy(1,action1) = policy(1,action1) + betaR * delta;
            values(1) = values(1) + alphaR * delta;
            
            % UPDATE MODEL-FREE
            if reward >= 0
                % Second move
                delta = reward - values(state2);
                policy(state2,action2) = policy(state2,action2) + betaR*delta;
                values(state2) = values(state2) + alphaR*delta;
                
                % Elig trace
                values(1) = values(1) + eligR * alphaR * delta;
                policy(1,action1) = policy(1,action1) + eligR*betaR*delta;
            else
                delta = reward - values(state2);
                policy(state2,action2) = policy(state2,action2) + betaP*delta;
                values(state2) = values(state2) + alphaP*delta;
                
                % Elig trace
                values(1) = values(1) + eligP * alphaP * delta;
                policy(1,action1) = policy(1,action1) + eligP*betaP*delta;
            end
            
            deltaR = gammaR*valuesR(state2)-valuesR(1);
            deltaP = gammaP*valuesP(state2)-valuesP(1);
            policyR(1,action1) = policyR(1,action1)+betaR*deltaR;
            policyP(1,action1) = policyP(1,action1)+betaP*deltaP;
            valuesR(1) = valuesR(1)+alphaR*deltaR;
            valuesP(1) = valuesP(1)+alphaP*deltaP;
            
            % Then 2nd level
            deltaR = max(reward,0)-valuesR(state2);
            deltaP = min(reward,0)-valuesP(state2);
            policyR(state2,action2) = policyR(state2,action2)+betaR*deltaR;
            policyP(state2,action2) = policyP(state2,action2)+betaP*deltaP;
            valuesR(state2) = valuesR(state2)+alphaR*deltaR;
            valuesP(state2) = valuesP(state2)+alphaP*deltaP;
            
            % Then eligibility trace
            valuesR(1) = valuesR(1)+eligR*alphaR*deltaR;
            valuesP(1) = valuesP(1)+eligP*alphaP*deltaP;
            
            prevActions = curActions;
        end
    end
end

likelihood = -likelihood; % for patternsearch (or fmincon)
end